=== ALL SCRAPED CONTENT FOR QUERY: 'official documentation for regression algorithms' ===

Timestamp: 20250406_134314

Number of results scraped: 5

=== RESULT 1: Understanding Regression Analysis in Machine Learning - H2O.ai ===
URL: https://h2o.ai/wiki/regression/

CONTENT:
--------------------------------------------------------------------------------
Understanding Regression Analysis in Machine Learning X Return to page Platform Generative AI Why H2O.ai End-to-end GenAI platform built for air-gapped, on-premises or cloud VPC deployments. Own every part of the stack--own your data and your prompts. Enterprise h2oGPTe Connect any LLM/embedding model, fully scalable w/K8s, includes guardrails, summarization, cost controls, and customization options. Open Source h2oGPT Customize and deploy open source AI models, create your own digital assistants and business GPTs. H2O Danube3 Open weight SLMs for on-device and offline applications. H2OVL Mississippi Open weight small vision-language models for OCR and Document AI. H2O Model Validation for LLMs Evaluation framework with automated testing, human calibration, bias detection, explainability, and failure analysis to boost compliance and risk control. H2O LLM Studio No-code fine-tuning for custom enterprise-grade LLMs. Train scalable SLMs for cheaper, more efficient NLP use cases. GenAI App Store Develop, deploy and share safe and trusted applications for your organization with use cases across enterprise, public sector, and more. Predictive AI H2O Driverless AI Democratizing AI with Automated Machine Learning H2O-3 Open Source Distributed Machine Learning H2O Document AI Extracting Data with Intelligence H2O Hydrogen Torch No-Code Deep Learning H2O Wave Open source low-code AI AppDev Framework H2O Label Genie AI-powered Data Labeling H2O AI Feature Store Infuse Your Data with Intelligence H2O MLOps Model Hosting, Monitoring and Deployment H2O AI AppStore Industry and Use Case AI Apps On-Premise Platform Choose to deploy on-premise and airgapped, self hosted on VPC, or fully hosted and managed by H2O.ai. Managed Cloud Hybrid Cloud Solutions Industry Solutions Financial Services Government Health Insurance Manufacturing Marketing Retail Telecommunications Use Cases Financial Services From Credit Scoring and Customer Churn to Anti-Money Laundering Government Use Responsible AI in Government Health From Clinical Workflow to Predicting ICU Transfers Insurance From Claims Management to Fraud Mitigation Manufacturing From Predictive Maintenance to Transportation Optimization Marketing From Content Personalization to Lead Scoring Retail From Assortment Optimization to Pricing Optimization Telecommunications From Predictive Customer Support to Predictive Fleet Maintenance View All H2O.ai Hospital Occupancy Simulator Track, predict, and manage COVID-19 related hospital admissions Strategic Transformation Use the H2O AI Cloud to make your company an AI company Customers View All Case Studies FINANCIAL SERVICES Learn how CBA is boosting AI capabilities to generate better customer and community outcomes, at greater pace and scale. TELECOM Learn how AT&T is transforming its call center operations with H2O.ai's Generative AI HEALTHCARE Learn how USCF Health is applying H2O Document AI to automate workflows in healthcare ENERGY Learn how AES is transforming its energy business with AI and H2O.ai FINANCIAL INDUSTRIES Learn now IFFCO-Tokio uses the H2O AI Cloud to save over $1M annually by transforming their fraud prediction processes MARKETING Learn how Epsilon is increasing its customers' marketing ROI with H2O.ai Partners Partners Find a Partner Become a Partner Powered by H2O.ai Partner University Resources Resources H2O University Documentation Resources Archive Wiki Customer Support Portal What is an AI Cloud? Research Papers Blog Open Source Downloads h2oGPT and H2O LLM H2O-3 H2O AutoML H2O Wave Sparkling Water Join H2O University Gain expertise through engaging courses and earn certifications to thrive on your AI journey. Support Get help and technology from the experts in H2O and access to Enterprise Team Events Events Events Webinar H2O GenAI World Make with H2O H2O.ai Wiki Read the H2O.ai wiki for up-to-date resources about artificial intelligence and machine learning. Responsible AI Learn the best practices for building responsible AI models and applications Company Company About Us Team Democratize AI Why GenAI With H2O.ai? AI4Conservation AI4Good Careers Contact Us News Press Releases Awards H2O AI 100 2024 Celebrating the top AI thought leaders of 2024 2024 Gartner® Magic Quadrant™ H2O.ai is recognized as a Visionary in 2024 Gartner® Magic Quadrant™ for Cloud AI Developer Services What is an AI Cloud? A high-scale elastic environment for the AI lifecycle Request Live Demo On-demand Demos Sign In X WIKI Topics Alphabetic H2O Wiki Algorithms Activation Function Confusion Matrix Convolutional Neural Networks Forward Propagation Generative Adversarial Network Gradient Descent Linear Regression Logistic Regression Machine Learning Algorithms Multilayer Perceptron Naive Bayes Neural Networking and Deep Learning RuleFit Stack Ensemble Word2Vec XGBoost Artificial Intelligence AI Engineer AI Ethics AI Governance AI Models AI Risk Management AI Winter AI in Cloud Computing Artificial General Intelligence Document AI Explainable AI NTrees Prediction Validation Sets BERT Attention Mechanism BERT Binary Classification Classify Token ([CLS]) Conversational Response Generation GLUE (General Language Understanding Evaluation) GPT (Generative Pre-Trained Transformers) Language Modeling Layer Normalization​ Mask Token ([MASK]) Probability Distribution Probing Classifiers SQuAD (Stanford Question Answering Dataset) Self-attention Separate token ([SEP]) Sequence-to-sequence Language Generation Sequential Text Spans Text Classification Text Generation Transformer Architecture WordPiece Data Big Data Citizen Data Scientist Data Profiling Data Science Shapley Values Structured vs Unstructured Data Time Series Data Deep Learning Deep Learning Cloud Deep Learning Use Cases Differentiable Programming Reinforcement Learning General Feature Engineering Feature Selection Machine Learning Operations Machine Learning Automated Machine Learning Hyperparameter Optimization ML Models Machine Learning Machine Learning Lifecycle Multiclass Classification Overfitting Python AutoML Supervised Machine Learning Training Sets Unsupervised Machine Learning Vector Modeling Back Propagation Classification Clustering Decision Tree Generalized Linear Models Model Fitting Neural Network Neural Network Architecture Operationalizing AI Random Forest Recurrent Network Regression Regression Trees Risk Governance Framework Underfitting cross-validation Predictions AUC-ROC Analytical Review Autoencoders Bias-Variance Tradeoff Decision Optimization Explanatory Variables Exponential Smoothing Level of Granularity Long Short-Term Memory Loss Function Model Management Precision and Recall Predictive Learning ROC Curve Recommendation system Stochastic Gradient Descent Target Leakage Target Variable Underwriting Tools Containers Natural Language Processing NumPy Optical Character Recognition Pytorch Sentiment Analysis Speech-to-Text Weights and Biases Training Artifacts Transfer Learning H2O Wiki A AI Engineer AI Ethics AI Governance AI Models AI Risk Management AI Winter AI in Cloud Computing AUC-ROC Activation Function Analytical Review Artifacts Artificial General Intelligence Attention Mechanism Autoencoders Automated Machine Learning B BERT Back Propagation Bias-Variance Tradeoff Big Data Binary Classification C Citizen Data Scientist Classification Classify Token ([CLS]) Clustering Confusion Matrix Containers Conversational Response Generation Convolutional Neural Networks cross-validation D Data Profiling Data Science Decision Optimization Decision Tree Deep Learning Cloud Deep Learning Use Cases Differentiable Programming Document AI E Explainable AI Explanatory Variables Exponential Smoothing F Feature Engineering Feature Selection Forward Propagation G GLUE (General Language Understanding Evaluation) GPT (Generative Pre-Trained Transformers) Generalized Linear Models Generative Adversarial Network Gradient Descent H Hyperparameter Optimization L Language Modeling Layer Normalization​ Level of Granularity Linear Regression Logistic Regression Long Short-Term Memory Loss Function M ML Models Machine Learning Machine Learning Algorithms Machine Learning Lifecycle Machine Learning Operations Mask Token ([MASK]) Model Fitting Model Management Multiclass Classification Multilayer Perceptron N NTrees Naive Bayes Natural Language Processing Neural Network Neural Network Architecture Neural Networking and Deep Learning NumPy O Operationalizing AI Optical Character Recognition Overfitting P Precision and Recall Prediction Predictive Learning Probability Distribution Probing Classifiers Python AutoML Pytorch R ROC Curve Random Forest Recommendation system Recurrent Network Regression Regression Trees Reinforcement Learning Risk Governance Framework RuleFit S SQuAD (Stanford Question Answering Dataset) Self-attention Sentiment Analysis Separate token ([SEP]) Sequence-to-sequence Language Generation Sequential Text Spans Shapley Values Speech-to-Text Stack Ensemble Stochastic Gradient Descent Structured vs Unstructured Data Supervised Machine Learning T Target Leakage Target Variable Text Classification Text Generation Time Series Data Training Sets Transfer Learning Transformer Architecture U Underfitting Underwriting Unsupervised Machine Learning V Validation Sets Vector W Weights and Biases Word2Vec WordPiece X XGBoost Wiki Topics -Select- H2O Wiki Algorithms Activation Function Confusion Matrix Convolutional Neural Networks Forward Propagation Generative Adversarial Network Gradient Descent Linear Regression Logistic Regression Machine Learning Algorithms Multilayer Perceptron Naive Bayes Neural Networking and Deep Learning RuleFit Stack Ensemble Word2Vec XGBoost Artificial Intelligence AI Engineer AI Ethics AI Governance AI Models AI Risk Management AI Winter AI in Cloud Computing Artificial General Intelligence Document AI Explainable AI NTrees Prediction Validation Sets BERT Attention Mechanism BERT Binary Classification Classify Token ([CLS]) Conversational Response Ge
--------------------------------------------------------------------------------

=== RESULT 2: Linear regression | Machine Learning - Google for Developers ===
URL: https://developers.google.com/machine-learning/crash-course/linear-regression

CONTENT:
--------------------------------------------------------------------------------
Home Products Machine Learning ML Concepts Crash Course Send feedback Linear regression Stay organized with collections Save and categorize content based on your preferences. Estimated module length: 70 minutes This module introduces linear regression concepts. Learning objectives: Explain a loss function and how it works. Define and describe how gradient descent finds the optimal model parameters. Describe how to tune hyperparameters to efficiently train a linear model. Prerequisites: This module assumes you are familiar with the concepts covered in the following module: Introduction to Machine Learning Linear regression is a statistical technique used to find the relationship between variables. In an ML context, linear regression finds the relationship between features and a label . For example, suppose we want to predict a car's fuel efficiency in miles per gallon based on how heavy the car is, and we have the following dataset: Pounds in 1000s (feature) Miles per gallon (label) 3.5 18 3.69 15 3.44 18 3.43 16 4.34 15 4.42 14 2.37 24 If we plotted these points, we'd get the following graph: Figure 1 . Car heaviness (in pounds) versus miles per gallon rating. As a car gets heavier, its miles per gallon rating generally decreases. We could create our own model by drawing a best fit line through the points: Figure 2 . A best fit line drawn through the data from the previous figure. Linear regression equation In algebraic terms, the model would be defined as $ y = mx + b $, where $ y $ is miles per gallon—the value we want to predict. $ m $ is the slope of the line. $ x $ is pounds—our input value. $ b $ is the y-intercept. In ML, we write the equation for a linear regression model as follows: $$ y' = b + w_1x_1 $$ where: $ y' $ is the predicted label—the output. $ b $ is the bias of the model. Bias is the same concept as the y-intercept in the algebraic equation for a line. In ML, bias is sometimes referred to as $ w_0 $. Bias is a parameter of the model and is calculated during training. $ w_1 $ is the weight of the feature. Weight is the same concept as the slope $ m $ in the algebraic equation for a line. Weight is a parameter of the model and is calculated during training. $ x_1 $ is a feature —the input. During training, the model calculates the weight and bias that produce the best model. Figure 3 . Mathematical representation of a linear model. In our example, we'd calculate the weight and bias from the line we drew. The bias is 30 (where the line intersects the y-axis), and the weight is -3.6 (the slope of the line). The model would be defined as $ y' = 30 + (-3.6)(x_1) $, and we could use it to make predictions. For instance, using this model, a 4,000-pound car would have a predicted fuel efficiency of 15.6 miles per gallon. Figure 4 . Using the model, a 4,000-pound car has a predicted fuel efficiency of 15.6 miles per gallon. Models with multiple features Although the example in this section uses only one feature—the heaviness of the car—a more sophisticated model might rely on multiple features, each having a separate weight ($ w_1 $, $ w_2 $, etc.). For example, a model that relies on five features would be written as follows: $ y' = b + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 $ For example, a model that predicts gas mileage could additionally use features such as the following: Engine displacement Acceleration Number of cylinders Horsepower This model would be written as follows: Figure 5 . A model with five features to predict a car's miles per gallon rating. By graphing some of these additional features, we can see that they also have a linear relationship to the label, miles per gallon: Figure 6 . A car's displacement in cubic centimeters and its miles per gallon rating. As a car's engine gets bigger, its miles per gallon rating generally decreases. Figure 7 . A car's acceleration and its miles per gallon rating. As a car's acceleration takes longer, the miles per gallon rating generally increases. Figure 8 . A car's horsepower and its miles per gallon rating. As a car's horsepower increases, the miles per gallon rating generally decreases. Exercise: Check your understanding What parts of the linear regression equation are updated during training? The bias and weights During training, the model updates the bias and weights. The prediction Predictions are not updated during training. The feature values Feature values are part of the dataset, so they're not updated during training. Key terms: Bias Feature Label Linear regression Parameter Weight Help Center Previous arrow_back Exercises Next Loss (10 min) arrow_forward Send feedback
--------------------------------------------------------------------------------

=== RESULT 3: 1. Supervised learning — scikit-learn 1.6.1 documentation ===
URL: https://scikit-learn.org/stable/supervised_learning.html

CONTENT:
--------------------------------------------------------------------------------
1. Supervised learning # 1.1. Linear Models 1.1.1. Ordinary Least Squares 1.1.2. Ridge regression and classification 1.1.3. Lasso 1.1.4. Multi-task Lasso 1.1.5. Elastic-Net 1.1.6. Multi-task Elastic-Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.11. Logistic regression 1.1.12. Generalized Linear Models 1.1.13. Stochastic Gradient Descent - SGD 1.1.14. Perceptron 1.1.15. Passive Aggressive Algorithms 1.1.16. Robustness regression: outliers and modeling errors 1.1.17. Quantile Regression 1.1.18. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage and Covariance Estimator 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.7. Mathematical formulation 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Online One-Class SVM 1.5.4. Stochastic Gradient Descent for sparse data 1.5.5. Complexity 1.5.6. Stopping criterion 1.5.7. Tips on Practical Use 1.5.8. Mathematical formulation 1.5.9. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.5. Nearest Centroid Classifier 1.6.6. Nearest Neighbors Transformer 1.6.7. Neighborhood Components Analysis 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. Gaussian Process Classification (GPC) 1.7.3. GPC examples 1.7.4. Kernels for Gaussian Processes 1.8. Cross decomposition 1.8.1. PLSCanonical 1.8.2. PLSSVD 1.8.3. PLSRegression 1.8.4. Canonical Correlation Analysis 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Complement Naive Bayes 1.9.4. Bernoulli Naive Bayes 1.9.5. Categorical Naive Bayes 1.9.6. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.8. Missing Values Support 1.10.9. Minimal Cost-Complexity Pruning 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking 1.11.1. Gradient-boosted trees 1.11.2. Random forests and other randomized tree ensembles 1.11.3. Bagging meta-estimator 1.11.4. Voting Classifier 1.11.5. Voting Regressor 1.11.6. Stacked generalization 1.11.7. AdaBoost 1.12. Multiclass and multioutput algorithms 1.12.1. Multiclass classification 1.12.2. Multilabel classification 1.12.3. Multiclass-multioutput classification 1.12.4. Multioutput regression 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.5. Sequential Feature Selection 1.13.6. Feature selection as part of a pipeline 1.14. Semi-supervised learning 1.14.1. Self Training 1.14.2. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.16.1. Calibration curves 1.16.2. Calibrating a classifier 1.16.3. Usage 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Tips on Practical Use 1.17.8. More control with warm_start
--------------------------------------------------------------------------------

=== RESULT 4: Artificial intelligence (AI) algorithms: a complete overview | Tableau ===
URL: https://www.tableau.com/data-insights/ai/algorithms

CONTENT:
--------------------------------------------------------------------------------
In this age of rapid technological advancement, most people are familiar with AI. We’ve likely all read the articles about automation destroying jobs, or a future of robots taking over the world. While those articles are more based on science fiction than a tangible future of AI, it certainly keeps interest in AI top-of-mind for many people. There are dozens of examples of AI that everyday consumers may use, such as facial recognition, auto-correct, search engines, or social media algorithms. But have you ever wondered how these programs work? AI runs off of algorithms, but not all AI algorithms are the same. They’re developed with different goals and methods. In this article, we’ll talk about the four major categories of AI algorithms and how they all work. In this article, we’ll cover: What is artificial intelligence? What is an AI algorithm? How do AI algorithms work? Types of AI algorithms Supervised learning Unsupervised learning Both supervised and unsupervised learning Reinforcement learning Uses of AI algorithms AI algorithms and business What is artificial intelligence? Artificial intelligence is a branch of computer science concerned with creating machines that can think and make decisions independently of human intervention. Those AI programs can do complex tasks that were previously only able to be done by humans. Some AI programs can complete simple tasks, some more complex. Some can take in data to learn and improve, completely without the touch of a human developer. Learn more about artificial intelligence . What is an AI algorithm? So then what is an AI algorithm? The definition of an algorithm is “a set of instructions to be followed in calculations or other operations.” This applies to both mathematics and computer science. So, at the essential level, an AI algorithm is the programming that tells the computer how to learn to operate on its own. An AI algorithm is much more complex than what most people learn about in algebra, of course. A complex set of rules drive AI programs, determining their steps and their ability to learn. Without an algorithm, AI wouldn’t exist. How do AI algorithms work? While a general algorithm can be simple, AI algorithms are by nature more complex. AI algorithms work by taking in training data that helps the algorithm to learn. How that data is acquired and is labeled marks the key difference between different types of AI algorithms. At the core level, an AI algorithm takes in training data (labeled or unlabeled, supplied by developers, or acquired by the program itself) and uses that information to learn and grow. Then it completes its tasks, using the training data as a basis. Some types of AI algorithms can be taught to learn on their own and take in new data to change and refine their process. Others will need the intervention of a programmer in order to streamline. Types of artificial intelligence algorithms There are three major categories of AI algorithms: supervised learning, unsupervised learning, and reinforcement learning. The key differences between these algorithms are in how they’re trained, and how they function. Under those categories, there are dozens of different algorithms. We’ll be talking about the most popular and commonly used from each category, as well as where they are commonly used. Supervised learning algorithms The first, and most commonly used category of algorithms is “Supervised learning.” These work by taking in clearly-labeled data while being trained and using that to learn and grow. It uses the labeled data to predict outcomes for other data. The name “supervised learning” comes from the comparison of a student learning in the presence of a teacher or expert. Building a supervised learning algorithm that actually works takes a team of dedicated experts to evaluate and review the results, not to mention data scientists to test the models the algorithm creates to ensure their accuracy against the original data, and catch any errors from the AI. Definitions: Classification and Regression Below, we jump into explaining the different types of supervised learning algorithms. All of them can either be used for classification or regression, or both. Classification means an either/or result using binary (0 = no, 1 = yes). So the algorithm will classify something as either one or another, but never both. There is also multi-class classification , which deals with organizing data into defined categories or types relevant to a specific need. Regression means the result will end with a real number (either round or a decimal point). You usually have a dependent variable and an independent variable, and the algorithm will use both points to estimate a possible other result (either forecast or generalized estimate). Decision Tree One of the most common supervised learning algorithms, decision trees get their name because of their tree-like structure (even though the tree is inverted). The “roots” of the tree are the training datasets and they lead to specific nodes which denote a test attribute. Nodes often lead to other nodes, and a node that doesn’t lead onward is called a “leaf”. Decision trees classify all the data into decision nodes. It uses a selection criteria called Attribute Selection Measures (ASM) which takes into account various measures (some examples would be entropy, gain ratio, information gain, etc). Using the root data and following the ASM, the decision tree can classify the data it is given by following the training data into sub-nodes until it reaches the conclusion. Random Forest The random forest algorithm is actually a broad collection of different decision trees, leading to its name. The random forest builds different decision trees and connects them to gain more accurate results. These can be used for both classification and regression. Support Vector Machines The support vector machine (SVM) algorithm is another common AI algorithm that can be used for either classification or regression (but is most often used for classification). SVM works by plotting each piece of data on a chart (in N dimensional space where N = the number of datapoints). Then, the algorithm classifies the datapoints by finding the hyperplace that separates each class. There can be more than one hyperplane. Naive Bayes The reason this algorithm is called “Naive Bayes” is that it’s based on Bayes’ Theorem, and also relies heavily on a large assumption: that the presence of one particular feature is unrelated to the presence of other features in the same class. That major assumption is the “naive” aspect of the name. Naive Bayes is useful for large datasets with many different classes. It, like many other supervised learning algorithms, is a classification algorithm. Linear regression Linear regression is a supervised learning AI algorithm used for regression modeling. It’s mostly used for discovering the relationship between data points, predictions, and forecasting. Much like SVM, it works by plotting pieces of data on a chart with the X-axis is the independent variable and the Y-axis is the dependent variable. The data points are then plotted out in a linear fashion to determine their relationship and forecast possible future data. Logistic regression A logistic regression algorithm usually uses a binary value (0/1) to estimate values from a set of independent variables. The output of logistic regression is either 1 or 0, yes or no. An example of this would be a spam filter in email. The filter uses logistic regression to mark whether an incoming email is spam (0) or not (1). Logistic regression is only useful when the dependent variable is categorical, either yes or no. Create beautiful visualizations with your data. Try Tableau for free Unsupervised learning algorithms It may at this point be relatively easy to guess what unsupervised learning algorithms mean, in comparison to supervised learning. Unsupervised learning algorithms are given data that isn’t labeled. Unsupervised learning algorithms use that unlabeled data to create models and evaluate the relationships between different data points in order to give more insight to the data. Definition: Clustering Many unsupervised learning algorithms perform the function of clustering, which means they sort the unlabeled data points into pre-defined clusters. The goal is to have each data point belong to only one cluster, with no overlap. There can be more than one data point in any given cluster, but a data point cannot belong to more than one cluster. K-means clustering K-means is an algorithm designed to perform the clustering function in unsupervised learning. It does this by taking in the pre-determined clusters and plotting out all the data regardless of the cluster. It then plots a randomly-selected piece of data as the centroid for each cluster (think of it as a circle around each cluster, with that piece of data as the exact center point). From there, it sorts the remaining data points into clusters based on their proximity to each other and the centroid data point for each cluster. Gaussian mixture model Gaussian mixture models are similar to K-means clustering in many ways. Both are concerned with sorting data into pre-determined clusters based on proximity. However, Gaussian models are a little more versatile in the shapes of the clusters they allow. Picture a graph with all your data points plotted out. K-means clustering only allows data to be clustered in circles with the centroid in the center of each cluster. Gaussian mixture can handle data that lands on the graph in more linear patterns, allowing for oblong-shaped clusters. This allows for greater clarity in clustering if one datapoint lands inside the circle of another cluster. Both supervised and unsupervised algorithms Some AI algorithms can use either supervised or unsupervised data input and still function. They might have slightly different applications based on their status. K-nearest neighbor algorithm K-nearest neigh
--------------------------------------------------------------------------------

=== RESULT 5: Neural Network Regression component - machine-learning - GitHub ===
URL: https://github.com/MicrosoftDocs/azure-ai-docs/blob/main/articles/machine-learning/component-reference/neural-network-regression.md

CONTENT:
--------------------------------------------------------------------------------
title titleSuffix description services ms.service ms.subservice ms.topic author ms.author ms.date Neural Network Regression: Component Reference Azure Machine Learning Learn how to use the Neural Network Regression component in Azure Machine Learning to create a regression model using a customizable neural network algorithm.. machine-learning azure-machine-learning core reference likebupt keli19 04/22/2020 Neural Network Regression component Creates a regression model using a neural network algorithm Category: Machine Learning / Initialize Model / Regression Component overview This article describes a component in Azure Machine Learning designer. Use this component to create a regression model using a customizable neural network algorithm. Although neural networks are widely known for use in deep learning and modeling complex problems such as image recognition, they are easily adapted to regression problems. Any class of statistical models can be termed a neural network if they use adaptive weights and can approximate non-linear functions of their inputs. Thus neural network regression is suited to problems where a more traditional regression model cannot fit a solution. Neural network regression is a supervised learning method, and therefore requires a tagged dataset , which includes a label column. Because a regression model predicts a numerical value, the label column must be a numerical data type. You can train the model by providing the model and the tagged dataset as an input to Train Model . The trained model can then be used to predict values for the new input examples. Configure Neural Network Regression Neural networks can be extensively customized. This section describes how to create a model using two methods: Create a neural network model using the default architecture If you accept the default neural network architecture, use the Properties pane to set parameters that control the behavior of the neural network, such as the number of nodes in the hidden layer, learning rate, and normalization. Start here if you are new to neural networks. The component supports many customizations, as well as model tuning, without deep knowledge of neural networks. Define a custom architecture for a neural network Use this option if you want to add extra hidden layers, or fully customize the network architecture, its connections, and activation functions. This option is best if you are already somewhat familiar with neural networks. You use the Net# language to define the network architecture. Create a neural network model using the default architecture Add the Neural Network Regression component to your pipeline in the designer. You can find this component under Machine Learning , Initialize , in the Regression category. Indicate how you want the model to be trained, by setting the Create trainer mode option. Single Parameter : Choose this option if you already know how you want to configure the model. Parameter Range : Select this option if you are not sure of the best parameters, and want to run a parameter sweep. Select a range of values to iterate over, and the Tune Model Hyperparameters iterates over all possible combinations of the settings you provided to determine the hyperparameters that produce the optimal results. In Hidden layer specification , select Fully connected case . This option creates a model using the default neural network architecture, which for a neural network regression model, has these attributes: The network has exactly one hidden layer. The output layer is fully connected to the hidden layer and the hidden layer is fully connected to the input layer. The number of nodes in the hidden layer can be set by the user (default value is 100). Because the number of nodes in the input layer is determined by the number of features in the training data, in a regression model there can be only one node in the output layer. For Number of hidden nodes , type the number of hidden nodes. The default is one hidden layer with 100 nodes. (This option is not available if you define a custom architecture using Net#.) For Learning rate , type a value that defines the step taken at each iteration, before correction. A larger value for learning rate can cause the model to converge faster, but it can overshoot local minima. For Number of learning iterations , specify the maximum number of times the algorithm processes the training cases. For The momentum , type a value to apply during learning as a weight on nodes from previous iterations. Select the option, Shuffle examples , to change the order of cases between iterations. If you deselect this option, cases are processed in exactly the same order each time you run the pipeline. For Random number seed , you can optionally type a value to use as the seed. Specifying a seed value is useful when you want to ensure repeatability across runs of the same pipeline. Connect a training dataset and train the model: If you set Create trainer mode to Single Parameter , connect a tagged dataset and the Train Model component. If you set Create trainer mode to Parameter Range , connect a tagged dataset and train the model by using Tune Model Hyperparameters . [!NOTE] If you pass a parameter range to Train Model , it uses only the default value in the single parameter list. If you pass a single set of parameter values to the Tune Model Hyperparameters component, when it expects a range of settings for each parameter, it ignores the values, and uses the default values for the learner. If you select the Parameter Range option and enter a single value for any parameter, that single value you specified is used throughout the sweep, even if other parameters change across a range of values. Submit the pipeline. Results After training is complete: To save a snapshot of the trained model, select the Outputs tab in the right panel of the Train model component. Select the Register dataset icon to save the model as a reusable component. Next steps See the set of components available to Azure Machine Learning.
--------------------------------------------------------------------------------

