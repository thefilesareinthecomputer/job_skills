=== SCRAPED CONTENT FROM: Learn to Build a Neural Network From Scratch — Yes, Really. ===

URL: https://medium.com/@waadlingaadil/learn-to-build-a-neural-network-from-scratch-yes-really-cac4ca457efc

CONTENT:
--------------------------------------------------------------------------------
Learn to Build a Neural Network From Scratch — Yes, Really. Aadil Mallick · Follow 77 min read · Sep 18, 2024 -- 14 Listen Share What the notation for a neural network looks like In this massive 76 minute tutorial, we’re going to build a neural network from scratch and understand all the math along the way. I’ve made this guide free because it’s what my younger self would have wanted. This article is meant for all kinds of people. Whether you’re interested in machine learning yet never coded a day in your life, a seasoned expert in deep learning yet never got down to the nitty gritty, or even a parrot — ok, maybe not that one — today you’ll finally learn to build a neural network. Building a neural network from scratch is the one coding exercise that makes you 100% better as a developer or engineer of any kind for these three reasons: You become more familiar with difficult math. You understand how deep learning works on a deep (excuse the pun) level. You understand how to make code more efficient using vectorization. I’m assuming you’re a total beginner, so this will be a long, long tutorial. The only prerequisite you need is to be able to solve the equation below because I don’t have an eternity to explain all of Algebra. If you can’t solve this, go back to watching Paw Patrol. But anyway, feel free to skip to any sections you care about and skip the fundamental sections marked below if you already have some machine learning (ML) knowledge. Fundamentals: What is machine learning? Fundamentals: Crash course on matrices Fundamentals: Crash course on derivatives Fundamentals: Crash course on partial derivatives The perceptron model Basic neural network notation Feed Forward Vectorization Cost Backpropagation Build that network Throughout this article, I’ll point you to external resources if you want to learn more about a given subject, because all I’m giving you is the barebone essentials. I’ve excluded everything that although is helpful, is unnecessary to build a strong neural network intuition. Read this article with this mindset: The code is not important. The concepts and math are. 1) What is machine learning? Photo by Arseny Togulev on Unsplash Have you ever wondered how ChatGPT seems like it’s able to understand you? Or how if you show a picture of a snake to Gemini, it can classify what kind of snake it is? Despite how seemingly human machines are, and how it would be impossible to fake that sort of knowledge, that is exactly what machines do — they fake it ‘till they make it. Think back to when you were a dumb baby who didn’t know anything about anything. You learned how to classify people, animals, trees, etc., but how? If you were born into a white family, the only people you interacted with were your parents. You must have thought all people were white, until you saw other living things. They had the same features as your parents, but had brown, black, or yellowish skin. They didn’t look like dogs, cats, or zebras. You had to expand your definition of human to encapsulate more and more people — short, tall, skinny, fat, with or without legs. You expanded your knowledge because you got more data . In a nutshell, that is exactly how a machine also learns. If you give a computer a picture of Jerry Seinfeld and classify him as a human being, the computer will think that Jerry Seinfeld is the only human that exists in the world. It will fail to classify any other person as a human being. But if you give the machine a picture of 100,000 human beings, telling each time “this is a human, this is a human,” then the computer will construct a broader definition for what a human looks like — face, arms, around 5 to 6 feet tall, wearing clothes, different skin colors, etc. Machines learn from data, just like humans do. The more data a machine has, the more its “worldview” and knowledge expands. Of course, the quality of the data matters. If you grow up telling a child that every banana they see is actually an apple, whenever they point to a banana, they’ll truly believe in their mind that the long yellow fruit is actually called an apple. How would a fellow classmate of this unfortunate child correct that behavior? Well, they would tell the kid, “dude, that’s a banana.” Correct that misguided spawn enough times, and eventually he’ll learn the correct names for bananas and apples. So what happened here? The child learned from his mistakes and errors and corrected them. Again, exactly like a machine learns. Machine learning consists of these steps: Gather correctly labeled data for the machine to train on. Create a metric to describe how much error the machine makes when trying to predict what something is. Iteratively train to reduce that error. Cost intuition When you first use a machine for machine learning, the predictions it makes are utter trash. So we “punish” the model just like how we “punish” a child by telling them that they’re wrong. But for machines, we take it one step further — we tell them how wrong they are, which we call the cost of the model. Let’s say we train a model to classify cats and dogs, and we test it by giving three images of dogs. It instead classifies all of them as cats. So how many did it get right? It got 0/3 right, so it obviously did pretty bad. But 1/3 is a better a score than 0/3, and 2/3 even more so. We can quantify the rate of errors through cost, with a basic implementation as follows: Cost = number of errors / number of total predictions So in the case of our super trash model, out of 3 predictions it made 3 wrong guesses, so the cost = 3/3 = 1, which is the highest cost for the model. All machine learning consists of is trying to minimize this cost metric as much as possible. A cost of 0 means we get 0 wrong predictions out of a total 3 predictions (0/3), which makes our model get everything right. Just understand this: Low cost = good model, high cost = bad model You can think of cost as synonymous to a grade — you get a D in a class, you don’t really know the material, but if you get an A, you’re doing swell. But how do we actually minimize this cost? That’s where the complicated math comes in, and we’ll learn that soon. Right now, let’s draw an analogy to neural networks. As the name implies, neural networks are trying to copy what the human brain does in hopes of creating artificial intelligence on par with that of humanity. A human brain has 100 billion neurons, so a machine model of the human brain (a neural network) will have 100 billion computing thingies, which we’ll call parameters for simplicity. If the cost if a measure of how poorly a model is doing, then it needs the output of the model first, which needs the 100 billion parameters to compute the final output. This makes the cost a behemoth of a function, taking in 100 billion variables and outputting a single number. Can you even minimize let alone understand a function of that size? Can you minimize the cost by hand? Here are the answers to both questions respectively: You can. You can’t — that’s why we use computers. Machine learning summary So now you know what machine learning is. When trying to get a machine learning model to classify stuff like images or data, we tell it what the right answers are and we test it on the data. From that testing, we get a cost metric for how poorly the model predicts stuff, and we do an iterative process involving complicated math to minimize that cost. When the cost is minimized, that means the model is predicting stuff correctly, almost as well as a human. 2) Crash Course on Matrices Matrices are a concise way to represent systems of equations. When you have to crunch several hundreds of numbers together (as you often have to do in machine learning), matrices are the key to making everything comprehensible. For example, how would you represent (2 * 1) + (3 * 5) + (2 * 2) concisely? Like so: Dot product Here we are multiplying two vectors (I’ll explain later) together in an operation called the dot product , which happens in these steps: For the first element in the first vector, multiply it with the first element in the second vector. For the second element in the first vector, multiply it with the second element in the second vector. For the third element in the first vector, multiply it with the third element in the second vector. Now sum up those three numbers, and that’s the dot product. You pair up each element from both vectors in order, multiply them, and then add up those resultant numbers, so let’s go through the math: First pair: 2*1 Second pair: 3*5 Third pair: 2*2 Added all up together, you get 2 + 15 + 4 = 21. So when you dot product two vectors together, you get back a single number sum, which we call a scalar . Keep in mind you cannot dot product two vectors together if they have different lengths . What if one vector has 2 elements and the second vector has three elements? That third element in the second vector has no other element to pair up with, and thus blows up and fries your calculator (ok, not that dramatic). Vectors are just one dimensional matrices, meaning they act just like a list of numbers. To gain a deep understanding about vectors, look at the following videos (not needed to build a network, but extremely helpful): Learn about what vectors represent here Matrices are more complicated. They are 2-dimensional, and can have many rows and columns. Here is an example of a matrix: A matrix with 2 rows and 2 columns So let’s review terminology: When you think of a scalar, think of a single number. When you think of a vector, think of a list of numbers. When you think of a matrix, think of a table of numbers . Why do we care so much about matrices? Because matrix multiplication can turn a lot of number crunching into simple expressions. For example, what if I want to multiply a lot of numbers together, but I don’t one final sum — I want a new matrix as a result? An example of multiplying two matrices together Matrix multiplication works as fol
--------------------------------------------------------------------------------

