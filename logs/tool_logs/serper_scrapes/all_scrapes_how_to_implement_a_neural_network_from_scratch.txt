=== ALL SCRAPED CONTENT FOR QUERY: 'how to implement a neural network from scratch' ===

Timestamp: 20250406_132817

Number of results scraped: 3

=== RESULT 1: Learn to Build a Neural Network From Scratch — Yes, Really. ===
URL: https://medium.com/@waadlingaadil/learn-to-build-a-neural-network-from-scratch-yes-really-cac4ca457efc

CONTENT:
--------------------------------------------------------------------------------
Learn to Build a Neural Network From Scratch — Yes, Really. Aadil Mallick · Follow 77 min read · Sep 18, 2024 -- 14 Listen Share What the notation for a neural network looks like In this massive 76 minute tutorial, we’re going to build a neural network from scratch and understand all the math along the way. I’ve made this guide free because it’s what my younger self would have wanted. This article is meant for all kinds of people. Whether you’re interested in machine learning yet never coded a day in your life, a seasoned expert in deep learning yet never got down to the nitty gritty, or even a parrot — ok, maybe not that one — today you’ll finally learn to build a neural network. Building a neural network from scratch is the one coding exercise that makes you 100% better as a developer or engineer of any kind for these three reasons: You become more familiar with difficult math. You understand how deep learning works on a deep (excuse the pun) level. You understand how to make code more efficient using vectorization. I’m assuming you’re a total beginner, so this will be a long, long tutorial. The only prerequisite you need is to be able to solve the equation below because I don’t have an eternity to explain all of Algebra. If you can’t solve this, go back to watching Paw Patrol. But anyway, feel free to skip to any sections you care about and skip the fundamental sections marked below if you already have some machine learning (ML) knowledge. Fundamentals: What is machine learning? Fundamentals: Crash course on matrices Fundamentals: Crash course on derivatives Fundamentals: Crash course on partial derivatives The perceptron model Basic neural network notation Feed Forward Vectorization Cost Backpropagation Build that network Throughout this article, I’ll point you to external resources if you want to learn more about a given subject, because all I’m giving you is the barebone essentials. I’ve excluded everything that although is helpful, is unnecessary to build a strong neural network intuition. Read this article with this mindset: The code is not important. The concepts and math are. 1) What is machine learning? Photo by Arseny Togulev on Unsplash Have you ever wondered how ChatGPT seems like it’s able to understand you? Or how if you show a picture of a snake to Gemini, it can classify what kind of snake it is? Despite how seemingly human machines are, and how it would be impossible to fake that sort of knowledge, that is exactly what machines do — they fake it ‘till they make it. Think back to when you were a dumb baby who didn’t know anything about anything. You learned how to classify people, animals, trees, etc., but how? If you were born into a white family, the only people you interacted with were your parents. You must have thought all people were white, until you saw other living things. They had the same features as your parents, but had brown, black, or yellowish skin. They didn’t look like dogs, cats, or zebras. You had to expand your definition of human to encapsulate more and more people — short, tall, skinny, fat, with or without legs. You expanded your knowledge because you got more data . In a nutshell, that is exactly how a machine also learns. If you give a computer a picture of Jerry Seinfeld and classify him as a human being, the computer will think that Jerry Seinfeld is the only human that exists in the world. It will fail to classify any other person as a human being. But if you give the machine a picture of 100,000 human beings, telling each time “this is a human, this is a human,” then the computer will construct a broader definition for what a human looks like — face, arms, around 5 to 6 feet tall, wearing clothes, different skin colors, etc. Machines learn from data, just like humans do. The more data a machine has, the more its “worldview” and knowledge expands. Of course, the quality of the data matters. If you grow up telling a child that every banana they see is actually an apple, whenever they point to a banana, they’ll truly believe in their mind that the long yellow fruit is actually called an apple. How would a fellow classmate of this unfortunate child correct that behavior? Well, they would tell the kid, “dude, that’s a banana.” Correct that misguided spawn enough times, and eventually he’ll learn the correct names for bananas and apples. So what happened here? The child learned from his mistakes and errors and corrected them. Again, exactly like a machine learns. Machine learning consists of these steps: Gather correctly labeled data for the machine to train on. Create a metric to describe how much error the machine makes when trying to predict what something is. Iteratively train to reduce that error. Cost intuition When you first use a machine for machine learning, the predictions it makes are utter trash. So we “punish” the model just like how we “punish” a child by telling them that they’re wrong. But for machines, we take it one step further — we tell them how wrong they are, which we call the cost of the model. Let’s say we train a model to classify cats and dogs, and we test it by giving three images of dogs. It instead classifies all of them as cats. So how many did it get right? It got 0/3 right, so it obviously did pretty bad. But 1/3 is a better a score than 0/3, and 2/3 even more so. We can quantify the rate of errors through cost, with a basic implementation as follows: Cost = number of errors / number of total predictions So in the case of our super trash model, out of 3 predictions it made 3 wrong guesses, so the cost = 3/3 = 1, which is the highest cost for the model. All machine learning consists of is trying to minimize this cost metric as much as possible. A cost of 0 means we get 0 wrong predictions out of a total 3 predictions (0/3), which makes our model get everything right. Just understand this: Low cost = good model, high cost = bad model You can think of cost as synonymous to a grade — you get a D in a class, you don’t really know the material, but if you get an A, you’re doing swell. But how do we actually minimize this cost? That’s where the complicated math comes in, and we’ll learn that soon. Right now, let’s draw an analogy to neural networks. As the name implies, neural networks are trying to copy what the human brain does in hopes of creating artificial intelligence on par with that of humanity. A human brain has 100 billion neurons, so a machine model of the human brain (a neural network) will have 100 billion computing thingies, which we’ll call parameters for simplicity. If the cost if a measure of how poorly a model is doing, then it needs the output of the model first, which needs the 100 billion parameters to compute the final output. This makes the cost a behemoth of a function, taking in 100 billion variables and outputting a single number. Can you even minimize let alone understand a function of that size? Can you minimize the cost by hand? Here are the answers to both questions respectively: You can. You can’t — that’s why we use computers. Machine learning summary So now you know what machine learning is. When trying to get a machine learning model to classify stuff like images or data, we tell it what the right answers are and we test it on the data. From that testing, we get a cost metric for how poorly the model predicts stuff, and we do an iterative process involving complicated math to minimize that cost. When the cost is minimized, that means the model is predicting stuff correctly, almost as well as a human. 2) Crash Course on Matrices Matrices are a concise way to represent systems of equations. When you have to crunch several hundreds of numbers together (as you often have to do in machine learning), matrices are the key to making everything comprehensible. For example, how would you represent (2 * 1) + (3 * 5) + (2 * 2) concisely? Like so: Dot product Here we are multiplying two vectors (I’ll explain later) together in an operation called the dot product , which happens in these steps: For the first element in the first vector, multiply it with the first element in the second vector. For the second element in the first vector, multiply it with the second element in the second vector. For the third element in the first vector, multiply it with the third element in the second vector. Now sum up those three numbers, and that’s the dot product. You pair up each element from both vectors in order, multiply them, and then add up those resultant numbers, so let’s go through the math: First pair: 2*1 Second pair: 3*5 Third pair: 2*2 Added all up together, you get 2 + 15 + 4 = 21. So when you dot product two vectors together, you get back a single number sum, which we call a scalar . Keep in mind you cannot dot product two vectors together if they have different lengths . What if one vector has 2 elements and the second vector has three elements? That third element in the second vector has no other element to pair up with, and thus blows up and fries your calculator (ok, not that dramatic). Vectors are just one dimensional matrices, meaning they act just like a list of numbers. To gain a deep understanding about vectors, look at the following videos (not needed to build a network, but extremely helpful): Learn about what vectors represent here Matrices are more complicated. They are 2-dimensional, and can have many rows and columns. Here is an example of a matrix: A matrix with 2 rows and 2 columns So let’s review terminology: When you think of a scalar, think of a single number. When you think of a vector, think of a list of numbers. When you think of a matrix, think of a table of numbers . Why do we care so much about matrices? Because matrix multiplication can turn a lot of number crunching into simple expressions. For example, what if I want to multiply a lot of numbers together, but I don’t one final sum — I want a new matrix as a result? An example of multiplying two matrices together Matrix multiplication works as fol
--------------------------------------------------------------------------------

=== RESULT 2: Building a neural network FROM SCRATCH (no Tensorflow/Pytorch ... ===
URL: https://www.youtube.com/watch?v=w8yWXqWQYmU

CONTENT:
--------------------------------------------------------------------------------
Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math) - YouTube About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy & Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC
--------------------------------------------------------------------------------

=== RESULT 3: Constructing Neural Networks From Scratch: Part 1 | DigitalOcean ===
URL: https://www.digitalocean.com/community/tutorials/constructing-neural-networks-from-scratch

CONTENT:
--------------------------------------------------------------------------------
DigitalOcean vs. AWS Lightsail: Which Cloud Platform is Right for You? With the presence of popular deep learning frameworks such as TensorFlow, Keras, PyTorch, and other similar libraries, it has become much easier for a novice in the field to pick up the subject of neural networks at a faster pace. Although these frameworks provide you a path to solving the most complex computations within a few minutes, they donât require you to understand the true core concepts and intuition behind all the requirements. If you have the knowledge of how a specific function works, and how exactly you can make use of that function within your code block, you will solve most problems without too much difficulty. However, for anyone to truly appreciate the concept of neural networks and understand the complete working procedure, it becomes essential to learn how these artificial neural networks work from scratch. How can these neural networks solve these complex problems? Understanding how neural networks work and, subsequently, how they are constructed is a worthwhile endeavor for anybody interested in AI and deep learning. While we are restricting ourselves from using any kind of deep learning frameworks such as TensorFlow, Keras, or PyTorch, we will still make use of other useful libraries like NumPy for numerical matrix computations. With NumPy arrays, we can perform numerous complex computations that will mimic the effect of deep learning, and use that to build understanding of the procedural workflow of these neural networks. We will implement some neural networks designed to solve a fairly simplistic task with the help of these neural nets built from scratch. In order to follow along with this article, you will need experience with Python code, and a beginners understanding of Deep Learning. We will operate under the assumption that all readers have access to sufficiently powerful machines, so they can run the code provided. If you do not have access to a GPU, we suggest usingDigitalOcean GPU Droplets. For instructions on getting started with Python code, we recommendtrying this beginners guideto set up your system and preparing to run beginner tutorials. The topic of neural networks is one of the most intriguing within the domain of deep learning and the future of Artificial Intelligence. While the term artificial neural networks is only loosely inspired by the concept of biological neurons, there are a few noticeable similarities that to keep in mind when conceptualizing them. Much like with human neurons, one interesting aspect of using artificial neural networks is that we typically can determinewhatthey are doing, but there is often no explicit way to determinehowthey work to achieve the goal. While we can potentially answer some of the âwhatâ aspects, there are numerous discoveries to be made to fully understand everything we need to know about how the model is behaving. This is known as the âblack boxâ metaphor for deep learning, and it can be applied to a number of deep learning systems. That being said, many neural networks are interpretable enough that we can easily explain their purpose and methodology, and this is dependent on your use case. Artificial Intelligence is a humungous field in which deep learning and neural networks are just two of the subdomains. In this article, our primary objective is to dive more deeply into the concept of neural networks, and we will proceed to show how to construct an architecture from scratch without making use of some of the prominent and popular deep learning frameworks. Before we dwell on the implementation of the neural network from scratch, let us gain an intuitive understanding of their working procedure. Most mathematical operations can be linked through a function. A function is one of the most essential concepts through which a neural network can learn almost anything. Regardless of what the function does, a neural network can be created to approximate that function. This is known as the the Universal Approximation Theorem, and its this law that allows neural networks to work on such a huge variety of different challenges while also giving them their black box nature. Most real-world problems such as computer vision tasks and natural language processing tasks can also be associated with each other in the form of functions. For example, through functions, we can link the input of a few words to a particular output word and a bunch of images to their respective output image. Most concepts in math and real-world problems can be reframed as a function to frame a problem for which the desired neural network can find the appropriate solution. The term Artificial Neural Networks is now commonly referred to as Neural Networks, Neural Nets or nns. These neural nets are loosely inspired on biological neurons. It is important to again note that there is actually very little correlation between the neurons in living entities and the ones used to construct neural network architectures. Although the underlying working procedures of both these elements are quite different, they do share the trait that when these neural networks are combined together, they can solve complex tasks with relative ease. To understand the basic concept of how neural networks work, one of the critical mathematical concepts that we can use to help understand neural networks is the line equation, which is ây = mx + c.â The ây = mxâ part of the equation helps to manipulate the line to achieve the desired shape and values. The other value, the intercept âcâ, helps to vary the positioning of the line by displacing itâs intercept on the y axis. Refer to both the images to gain a more clear understanding of this basic concept. In terms of neural networks, Y = WX +B can be used to represent this equation. Y would represent the output values, âwâ represents the weights that need to be adjusted, âxâ represents the input values, and âbâ represents the values. By using this simple logic, neural networks can use the known information âbâ and âwâ to determine the value for âxâ. To understand this particular concept of weights and biases better, let us explore a simple code snippet as shown below and the resulting output. Using some input values, weights, and biases, we can compute an output with the dot product of the inputs with the transpose of the weights. To this resulting value, the respective biases are added to compute the desired values. The example below is quite simple, but sufficient for creating a basic understanding. However, we will cover more complex concepts in the next section and in upcoming articles. When neural networks are collectively combined together, they are able to learn through a training process with the help of backpropagation. The first step is the forward propagation that occurs by computing the necessary information at each layer until the output cell by using random weights. However, these random weights are usually never close to perfect, and the weights need to be adjusted to achieve a more desired result. Hence, backpropagation in neural networks is one of the more crucial aspects of its functionality. Backpropogation is where the weights are manipulated and adjusted, usually with the comparison of the output in hand and the expected output. We will look into these concepts further in the next section and upcoming articles. In this section, we will see how to solve some tasks with the help of the construction of neural networks from scratch. Before we start building our neural networks from scratch, let us gain an understanding of the type of problem that we are trying to solve in this article. Our objective is to construct neural networks that can understand and solve the functioning of logic gates, such as AND, OR, NOT, XOR, and other similar logic gates. For this specific example, we will look at how to solve the XOR gate problem by constructing our neural networks from scratch. Logic gates are some of the most elementary building blocks of electronic components. We are using these logic gates because, as their name suggests, each of these gates operates on a specific logic. For example, the XOR gate only provides a high output when both the input values are different. If both the input values are similar, the resulting output is low. These representations of logic are often represented in the form of a truth table. The image above shows the symbolic and truth table representation of the XOR gate. We can use the input and output values in the form of arrays to train our constructed neural network to achieve desirable results. Let us first import the necessary libraries that we will utilize for constructing neural networks from scratch. We will not use any deep learning frameworks in this section. We will only use the NumPy library to simplify some of the complex tensor computations and the overall mathematical calculations. You can choose to build neural networks even without the NumPy library, but it would be more time-consuming. We will also import the only other library we need for this section in matplotlib. We will use this library to visualize and plot the loss as we train our model for a specific number of epochs. Let us describe the inputs of the truth table and the expected output for the XOR gate. The readers can choose to work on different gates and experiment accordingly (Note that sometimes you not might get the desired results). Below is the code snippet for the input variables and expected outcome for the XOR gate. Let us combine the inputs together into a single array entity so that we have one total input array and one output array for the neural network to learn. This combining process can be done in a couple of ways. In the below code block, we are using a list to combine the two arrays and then converting the final list back into the numpy array format. In the next section, I have also mentioned another metho
--------------------------------------------------------------------------------

